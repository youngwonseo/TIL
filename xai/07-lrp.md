# LRP(Layer-wise Relevance Propagation)

* 계층별 타당성 전파(Layer-wise Relevance Propagation, LRP)는 딥러닝 모델의 결과를 역추적해서 입력 이미지에 히트맵(heatmap)을 출력하는 기법
* LRP는 히트맵 결과를 출력하는 딥러닝-XAI 기법중 가장 대표적인 방식
* LRP는 이론적으로 두 가지 기법이 혼합되어 있음
  * 타당성 전파(Relevance Propagation, RP) - 결과에 대한 원인을 분해하고 그 비중을 분배하는 과정
  * 분해(Decomposition) - RP로 얻어낸 원인을 가중치로 환원하고 해부하는 과정



* 딥러닝 모델은 많은 파라미터를 가진 활성화 함수들의 조합이어서 직관적으로 이해하기 어려운 경우가 대부분임
* 피처를 연결하고 활성화되는 과정이 비선형적(non-linear)이고, 다양한 커널로 매핑되기 때문에 추론 또한 어려움 
* 이러한 비선형성을 극복하고 설명가능하게 만들기 위해 필터시각화나 민감도를 사용하는 기법들이 등장
* 필터 시각화나 민감도는 딥러닝의 순방향(feed forward)으로 진행하며 데이터 흐름을 관찰하는 기법
* LRP는 딥러닝 모델이 분류한 결과를 **역순**으로 탐지하며 분해
* LRP는 결과에 대한 원본 이미지의 기여도를 표시
* LRP는 딥러닝 블랙박스를 역순으로 탐지하기 위해 **분해**기법을 사용, 기여도를 계산할 때 **타당성 전파**기법을 사용


## 분해(Decomposition)
* 분해는 블랙박스에 입력된 피처 하나가 결과 해석에 얼마나 영향을 미치는지 해제하는 방법
* 예를 들면 특정 픽셀 k가 결과를 도출하는데 도움이 되는지(+), 해가 되는지(-) 판단을 할 수 있음
* 딥러닝의 각 은닉층의 결과에 대한 기여도 판단

## 타당성 전파(Relevance Propagation, RP)
* 타당성 전파는 분해과정을 마친 은닉층이 결과값 출력에 어떤 기여를 하는지 타당성을 계산하는 방법
* 분해를 마치고 피처 타당성 계산으로 모든 은닉층 활성화 함수의 기여도를 계산할 수 있다면, **원본 이미지의 픽셀마다 기여도를 표시**할 수 있음



## 실습: 합성곱 신경망 속 열어보기


